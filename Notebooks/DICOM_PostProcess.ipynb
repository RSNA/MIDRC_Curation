{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (3.8.4)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (8.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: boto3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (1.34.95)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.95 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from boto3) (1.34.95)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from botocore<1.35.0,>=1.34.95->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from botocore<1.35.0,>=1.34.95->boto3) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.95->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install matplotlib ipywidgets\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import gc\n",
    "import pydicom\n",
    "from pydicom.datadict import dictionary_VR\n",
    "from pydicom.sequence import Sequence\n",
    "from pydicom.dataset import Dataset\n",
    "from pydicom.multival import MultiValue\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import botocore\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create log file\n",
    "def create_log_file(log_file_name):\n",
    "    logging.basicConfig(filename=log_file_name, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to prompt user for information\n",
    "def prompt_user():\n",
    "    name = input(\"Enter your name: \")\n",
    "    project_name = input(\"Enter project name: \")\n",
    "    log_date = input(\"Enter log file date (YYYY-MM-DD): \")\n",
    "    output_path = input(\"Enter the output directory path: \")\n",
    "    source_path = input(\"Enter the source directory path (local or S3 bucket): \")\n",
    "    return name, project_name, log_date, output_path, source_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append to project report CSV file\n",
    "def append_to_project_report(module_name, summary, output_path, user_name):\n",
    "    report_file = os.path.join(output_path, \"project_report.csv\")\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(report_file, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([now, user_name, module_name, summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 location handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_s3_path(path):\n",
    "    return path.startswith(\"s3://\")\n",
    "\n",
    "def parse_s3_path(s3_path):\n",
    "    # Assumes path format \"s3://bucket-name/path/to/object\"\n",
    "    if not is_s3_path(s3_path):\n",
    "        raise ValueError(f\"Invalid S3 path: {s3_path}\")\n",
    "    bucket_name = s3_path[5:].split('/')[0]\n",
    "    s3_key = '/'.join(s3_path[5:].split('/')[1:])\n",
    "    return bucket_name, s3_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions from the second notebook\n",
    "\n",
    "# AWS S3 Setup\n",
    "s3 = boto3.client('s3')\n",
    "# Function 1: Verify DICOM files\n",
    "def verify_dicom_files(directory, output_path, project_report_file=None):\n",
    "    \"\"\"\n",
    "    Verify DICOM files in a directory (local or S3 bucket).\n",
    "    Args:\n",
    "    - directory (str): The root directory to search for DICOM files.\n",
    "    - output_path (str): The path to save the output CSV file and log file.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(f\"Verifying DICOM files in directory: {directory}\")\n",
    "    \n",
    "    verification_results = []\n",
    "    \n",
    "    if is_s3_path(directory):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name, prefix = parse_s3_path(directory)\n",
    "        try:\n",
    "            paginator = s3.get_paginator('list_objects_v2')\n",
    "            page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "            \n",
    "            for page in page_iterator:\n",
    "                if \"Contents\" in page:\n",
    "                    for obj in page['Contents']:\n",
    "                        file_key = obj['Key']\n",
    "                        if file_key.endswith('.dcm'):\n",
    "                            obj_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "                            try:\n",
    "                                # Load DICOM file from S3\n",
    "                                obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "                                dicom_data = pydicom.dcmread(obj['Body'])\n",
    "                                verification_results.append({\"File\": obj_path, \"Verification\": \"Passed\"})\n",
    "                            except Exception as e:\n",
    "                                logging.error(f\"Error verifying DICOM file {obj_path}: {str(e)}\")\n",
    "                                verification_results.append({\"File\": obj_path, \"Verification\": \"Failed\"})\n",
    "        except NoCredentialsError as e:\n",
    "            logging.error(f\"Credentials error accessing S3: {str(e)}\")\n",
    "            print(f\"Credentials error: {str(e)}\")\n",
    "            return\n",
    "    else:\n",
    "        # Local directory processing\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        dicom_data = pydicom.dcmread(file_path)\n",
    "                        verification_results.append({\"File\": file_path, \"Verification\": \"Passed\"})\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error verifying DICOM file {file_path}: {str(e)}\")\n",
    "                        verification_results.append({\"File\": file_path, \"Verification\": \"Failed\"})\n",
    "    \n",
    "    verification_df = pd.DataFrame(verification_results)\n",
    "    verification_csv_file = os.path.join(output_path, \"dicom_verification_results.csv\")\n",
    "    verification_df.to_csv(verification_csv_file, index=False)\n",
    "    \n",
    "    logging.info(f\"Verification results saved to {verification_csv_file}\")\n",
    "    print(f\"Verification results saved to {verification_csv_file}\")\n",
    "    \n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Verify DICOM Files',\n",
    "                    'Summary': f\"Executed Verify DICOM Files {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "\n",
    "# Function 2: Check for duplicate SOP Instance UIDs\n",
    "def check_duplicate_sop_uids(directory, output_path, project_report_file=None):\n",
    "    \"\"\"\n",
    "    Check for duplicate SOP Instance UIDs in DICOM files within a directory (local or S3 bucket).\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(f\"Checking for duplicate SOP Instance UIDs in directory: {directory}\")\n",
    "    \n",
    "    duplicate_uids = defaultdict(list)\n",
    "    duplicates_dict = {}\n",
    "    \n",
    "    if is_s3_path(directory):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name, prefix = parse_s3_path(directory)\n",
    "        try:\n",
    "            paginator = s3.get_paginator('list_objects_v2')\n",
    "            page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "            \n",
    "            for page in page_iterator:\n",
    "                if \"Contents\" in page:\n",
    "                    for obj in page['Contents']:\n",
    "                        file_key = obj['Key']\n",
    "                        if file_key.endswith('.dcm'):\n",
    "                            obj_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "                            try:\n",
    "                                obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "                                dicom_data = pydicom.dcmread(obj['Body'])\n",
    "                                sop_instance_uid = dicom_data.SOPInstanceUID\n",
    "                                duplicate_uids[sop_instance_uid].append(obj_path)\n",
    "                                duplicates_dict.setdefault(sop_instance_uid, []).append(obj_path)\n",
    "                            except Exception as e:\n",
    "                                logging.error(f\"Error processing DICOM file {obj_path}: {str(e)}\")\n",
    "        except NoCredentialsError as e:\n",
    "            logging.error(f\"Credentials error accessing S3: {str(e)}\")\n",
    "            print(f\"Credentials error: {str(e)}\")\n",
    "            return\n",
    "    else:\n",
    "        # Local directory processing\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        dicom_data = pydicom.dcmread(file_path)\n",
    "                        sop_instance_uid = dicom_data.SOPInstanceUID\n",
    "                        duplicate_uids[sop_instance_uid].append(file_path)\n",
    "                        duplicates_dict.setdefault(sop_instance_uid, []).append(file_path)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Filter for duplicates\n",
    "    duplicates_dict = {key: value for key, value in duplicates_dict.items() if len(value) > 1}\n",
    "    \n",
    "    # Save results\n",
    "    duplicate_uids_csv_file = os.path.join(output_path, \"duplicate_sop_instance_uids.csv\")\n",
    "    with open(duplicate_uids_csv_file, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"SOPInstanceUID\", \"FilePaths\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for uid, file_paths in duplicates_dict.items():\n",
    "            writer.writerow({\"SOPInstanceUID\": uid, \"FilePaths\": \", \".join(file_paths)})\n",
    "    \n",
    "    logging.info(f\"Duplicate SOP Instance UIDs and corresponding paths saved to {duplicate_uids_csv_file}\")\n",
    "    print(f\"Duplicate SOP Instance UIDs and corresponding paths saved to {duplicate_uids_csv_file}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Check Duplicate SOP Instance UIDs',\n",
    "                    'Summary': f\"Executed Check Duplicate SOP Instance UIDs {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "    \n",
    "\n",
    "\n",
    "# Function 3: Check DICOM consistency\n",
    "\n",
    "def check_dicom_consistency(directory, output_path, project_report_file=None):\n",
    "    errors = defaultdict(list)\n",
    "\n",
    "    if is_s3_path(directory):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name, prefix = parse_s3_path(directory)\n",
    "        try:\n",
    "            paginator = s3.get_paginator('list_objects_v2')\n",
    "            page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "            for page in page_iterator:\n",
    "                for obj in page['Contents']:\n",
    "                    if obj['Key'].endswith('/') and obj['Key'] != prefix:  # A folder\n",
    "                        series_folder = obj['Key']\n",
    "                        dicom_files = [obj['Key'] for obj in page['Contents'] if obj['Key'].startswith(series_folder) and obj['Key'].endswith('.dcm')]\n",
    "                        process_dicom_files(dicom_files, series_folder, errors, s3, bucket_name)\n",
    "        except NoCredentialsError as e:\n",
    "            print(f\"Credentials error: {str(e)}\")\n",
    "            return\n",
    "    else:\n",
    "        for series_folder in os.listdir(directory):\n",
    "            series_path = os.path.join(directory, series_folder)\n",
    "            if os.path.isdir(series_path):\n",
    "                dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n",
    "                process_dicom_files([os.path.join(series_path, f) for f in dicom_files], series_folder, errors)\n",
    "\n",
    "    csv_file_path = os.path.join(output_path, 'dicom_consistency_errors.csv')\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['SeriesFolder', 'FilePath', 'Error'])\n",
    "        for series_folder, error_list in errors.items():\n",
    "            for error in error_list:\n",
    "                writer.writerow([series_folder, error[0], error[1]])\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Check DICOM Consistency',\n",
    "                    'Summary': f\"Executed Check DICOM Consistency: Checking DICOM consistency in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}\")\n",
    "\n",
    "    return csv_file_path\n",
    "\n",
    "def process_dicom_files(dicom_files, series_folder, errors, s3=None, bucket_name=None):\n",
    "    \"\"\"\n",
    "    Process a list of DICOM files to check for consistency within a series folder.\n",
    "\n",
    "    Args:\n",
    "    - dicom_files (list): List of DICOM files to be processed.\n",
    "    - series_folder (str): The name of the folder containing the DICOM series.\n",
    "    - errors (defaultdict): A dictionary to collect errors encountered during processing.\n",
    "    - s3 (boto3.client, optional): An optional S3 client object if files are on AWS S3.\n",
    "    - bucket_name (str, optional): The name of the S3 bucket if files are on AWS S3.\n",
    "\n",
    "    This function does not return a value but modifies the `errors` dictionary in place.\n",
    "    \"\"\"\n",
    "    attributes = defaultdict(list)\n",
    "    image_positions = []\n",
    "    for dicom_file in dicom_files:\n",
    "        try:\n",
    "            if s3:\n",
    "                # Retrieve the object from S3 and load it into a pydicom FileDataset\n",
    "                obj = s3.get_object(Bucket=bucket_name, Key=dicom_file)\n",
    "                dicom_data = pydicom.dcmread(obj['Body'])\n",
    "            else:\n",
    "                # Load DICOM file from local filesystem\n",
    "                dicom_data = pydicom.dcmread(dicom_file)\n",
    "            \n",
    "            # Check for the presence of essential DICOM attributes\n",
    "            essential_attributes = ['PatientID', 'StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID']\n",
    "            for attribute in essential_attributes:\n",
    "                if attribute not in dicom_data:\n",
    "                    errors[series_folder].append((dicom_file, f\"Missing {attribute} in DICOM file: {dicom_file.split('/')[-1]}\"))\n",
    "                else:\n",
    "                    attributes[attribute].append(dicom_data[attribute].value)\n",
    "\n",
    "            # Collect and check ImagePositionPatient attribute for consistency\n",
    "            if 'ImagePositionPatient' in dicom_data:\n",
    "                image_positions.append(dicom_data.ImagePositionPatient)\n",
    "            if len(image_positions) > 1 and len(set(image_positions)) != 1:\n",
    "                errors[series_folder].append((dicom_file, \"Inconsistent ImagePositionPatient values across DICOM files.\"))\n",
    "        except Exception as e:\n",
    "            errors[series_folder].append((dicom_file, f\"Error processing DICOM file {dicom_file.split('/')[-1]}: {str(e)}\"))\n",
    "\n",
    "# Function 4: Verify DICOM IOD data consistency\n",
    "\n",
    "# Modality-specific rules\n",
    "modality_rules = {\n",
    "    'CT': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['CT']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {\n",
    "            'SliceThickness': 'required'\n",
    "        }\n",
    "    },\n",
    "    'MRI': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['MR']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {}\n",
    "    },\n",
    "    'US': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['US']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {}\n",
    "    },\n",
    "    'PET': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['PT']), 'Manufacturer': 'LO'\n",
    "        }\n",
    "    },\n",
    "    'PT': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['PT']), 'Manufacturer': 'LO',  \n",
    "        },\n",
    "        'conditional': {\n",
    "            'FrameReferenceTime': 'required'\n",
    "        }\n",
    "    },\n",
    "    'DX': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['DX']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {\n",
    "            'Exposure': 'required'\n",
    "        }\n",
    "    },\n",
    "    'MR': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['MR']), 'Manufacturer': 'LO',\n",
    "            'MagneticFieldStrength': 'DS',  \n",
    "            'EchoTime': 'DS',  \n",
    "            'RepetitionTime': 'DS' \n",
    "        },\n",
    "        'conditional': {\n",
    "            'FlipAngle': 'required'\n",
    "        }\n",
    "    },\n",
    "    'RF': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['RF']), 'Manufacturer': 'LO',\n",
    "        },\n",
    "        'conditional': {\n",
    "            'FrameTime': 'required'\n",
    "        }\n",
    "    },\n",
    "    'XA': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['XA']), 'Manufacturer': 'LO',\n",
    "        },\n",
    "        'conditional': {\n",
    "            'ExposureTime': 'required'\n",
    "        }\n",
    "    },\n",
    "    'CR': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['CR']), 'Manufacturer': 'LO',\n",
    "        },\n",
    "        'conditional': {\n",
    "            'Exposure': 'required'\n",
    "        }\n",
    "    },\n",
    "    'NM': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['NM']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {\n",
    "            'FrameReferenceTime': 'required'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def validate_dicom_modality(ds, modality):\n",
    "    errors = []\n",
    "\n",
    "    # Check if the modality is recognized and in the dictionary\n",
    "    if modality not in modality_rules:\n",
    "        errors.append(f\"Unsupported or undefined modality: {modality}\")\n",
    "        return errors  # Return early as further validation doesn't make sense\n",
    "\n",
    "    # Get the rules associated with the modality\n",
    "    rules = modality_rules[modality]\n",
    "\n",
    "    for attr, attr_type in rules['required'].items():\n",
    "        if not hasattr(ds, attr):\n",
    "            if attr_type == \"UI\":\n",
    "                continue  # Silently skip attributes expected to be \"UI\"\n",
    "            errors.append(f\"Missing attribute: {attr}\")\n",
    "        else:\n",
    "            actual_value = getattr(ds, attr)\n",
    "\n",
    "            # Directly handle simple types\n",
    "            if isinstance(actual_value, str):\n",
    "                if isinstance(attr_type, tuple):\n",
    "                    expected_type, expected_values = attr_type\n",
    "                    if expected_type != \"LO\" and actual_value not in expected_values:\n",
    "                        errors.append(f\"Attribute {attr} has an incorrect value: {actual_value} not in {expected_values}\")\n",
    "                elif attr_type != \"LO\":\n",
    "                    if attr_type == \"UI\":\n",
    "                        continue  # Skip logging for this type mismatch\n",
    "                    errors.append(f\"Attribute {attr} has incorrect type: expected {attr_type}, got str\")\n",
    "            elif hasattr(actual_value, 'VR'):  # Complex DICOM element case\n",
    "                if actual_value.VR != attr_type:\n",
    "                    if attr_type == \"UI\":\n",
    "                        continue  # Silently skip this type mismatch\n",
    "                    errors.append(f\"Attribute {attr} has incorrect type: expected {attr_type}, got {actual_value.VR}\")\n",
    "                elif isinstance(attr_type, tuple):\n",
    "                    expected_values = attr_type[1]\n",
    "                    if actual_value.value not in expected_values:\n",
    "                        errors.append(f\"Attribute {attr} has an incorrect value: {actual_value.value} not in {expected_values}\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "def verify_dicom_iod_data(directory, output_path, location='local', project_report_file=None):\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    logging.info(f\"Verifying DICOM IOD data consistency in {location} directory: {directory}\")\n",
    "\n",
    "    iod_verification_report = {}\n",
    "\n",
    "    if location == 's3':\n",
    "        # Process files from an S3 bucket\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=directory):\n",
    "            for item in tqdm(page.get('Contents', [])):\n",
    "                file_key = item['Key']\n",
    "                if file_key.endswith('.dcm'):\n",
    "                    try:\n",
    "                        obj = s3.get_object(Bucket=directory, Key=file_key)\n",
    "                        dicom_data = pydicom.dcmread(BytesIO(obj['Body'].read()))\n",
    "\n",
    "                        modality = dicom_data.Modality if 'Modality' in dicom_data else None\n",
    "                        if modality:\n",
    "                            print(f\"Modality found: {modality}\")\n",
    "                        else:\n",
    "                            print(\"No Modality attribute found\")\n",
    "\n",
    "                        if \"PixelData\" not in dicom_data:\n",
    "                            logging.warning(f\"Missing PixelData in DICOM file: {file_key}\")\n",
    "                            iod_verification_report[file_key] = \"Missing PixelData\"\n",
    "                        else:\n",
    "                            errors = validate_dicom_modality(dicom_data, modality)\n",
    "                            if errors:\n",
    "                                iod_verification_report[file_key] = \", \".join(errors)\n",
    "                                logging.warning(f\"Verification issues in {file_key}: {', '.join(errors)}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing DICOM file {file_key}: {str(e)}\")\n",
    "    else:\n",
    "        # Process files from a local directory\n",
    "        for root, dirs, files in tqdm(os.walk(directory)):\n",
    "            for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        dicom_data = pydicom.dcmread(file_path)\n",
    "\n",
    "                        modality = dicom_data.Modality if 'Modality' in dicom_data else None\n",
    "                        if modality is None:\n",
    "                            print(\"No Modality attribute found\")\n",
    "\n",
    "                        if \"PixelData\" not in dicom_data:\n",
    "                            logging.warning(f\"Missing PixelData in DICOM file: {file_path}\")\n",
    "                            iod_verification_report[file_path] = \"Missing PixelData\"\n",
    "                        else:\n",
    "                            errors = validate_dicom_modality(dicom_data, modality)\n",
    "                            if errors:\n",
    "                                iod_verification_report[file_path] = \", \".join(errors)\n",
    "                                logging.warning(f\"Verification issues in {file_path}: {', '.join(errors)}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "\n",
    "    # Save verification report to a CSV file\n",
    "    verification_report_csv = os.path.join(output_path, \"iod_verification_report.csv\")\n",
    "    with open(verification_report_csv, \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"File\", \"Issue\"])\n",
    "        for file_path, issue in iod_verification_report.items():\n",
    "            writer.writerow([file_path, issue])\n",
    "\n",
    "    logging.info(f\"IOD verification report saved to {verification_report_csv}\")\n",
    "    print(f\"IOD verification report saved to {verification_report_csv}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=['Module', 'Summary', 'Timestamp'])\n",
    "                writer.writerow({\n",
    "                    'Module': 'Verify DICOM IOD Data Consistency',\n",
    "                    'Summary': f\"Executed Verify DICOM IOD Data Consistency in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}\")\n",
    "\n",
    "    return iod_verification_report\n",
    "\n",
    "# Function 5: Remove PHI info from DICOM metadata\n",
    "\n",
    "def convert_to_tuple(data):\n",
    "    \"\"\"Convert lists, sequences, datasets, and multivalued types to tuples recursively.\"\"\"\n",
    "    if isinstance(data, (Sequence, list, MultiValue)):\n",
    "        return tuple(convert_to_tuple(item) for item in data)\n",
    "    elif isinstance(data, Dataset):\n",
    "        # Convert dataset to a dictionary and process recursively\n",
    "        data_dict = {tag: convert_to_tuple(data.get(tag)) for tag in data.dir()}\n",
    "        return tuple(data_dict.items())\n",
    "    elif isinstance(data, dict):\n",
    "        return {key: convert_to_tuple(value) for key, value in data.items()}\n",
    "    return data\n",
    "\n",
    "def aggregate_dicom_metadata(directory, output_path, location='local', project_report_file=None, batch_size=2):\n",
    "    \"\"\"\n",
    "    Aggregate unique DICOM metadata from the source directory and save it to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): The root directory containing DICOM files.\n",
    "    - output_path (str): The path to save the aggregated metadata CSV file.\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "    - batch_size (int): The number of files to process before collecting garbage.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    logging.info(f\"Aggregating DICOM metadata from {location} directory: {directory}\")\n",
    "\n",
    "    # List of metadata parameters to exclude\n",
    "    exclude_parameters = [\n",
    "        \"AccessionNumber\", \"AcquisitionDate\", \"AcquisitionDateTime\", \"AcquisitionTime\",\n",
    "        \"Columns\", \"ContentTime\", \"ContentDate\", \"BluePaletteColorLookupTableData\",\n",
    "        \"AcquisitionMatrix\", \"CineRate\"\n",
    "    ]\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    aggregated_metadata_file = os.path.join(output_path, \"aggregated_dicom_metadata.csv\")\n",
    "    with open(aggregated_metadata_file, \"w\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Initialize a dictionary of sets to store unique values for each metadata parameter\n",
    "        metadata_dicts = defaultdict(set)\n",
    "\n",
    "        file_count = 0  # To track batch processing\n",
    "\n",
    "        # Iterate over all DICOM files in the directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        dicom_data = pydicom.dcmread(file_path)\n",
    "\n",
    "                        # Convert DICOM metadata to a dictionary\n",
    "                        dicom_dict = {tag: dicom_data.get(tag) for tag in dicom_data.dir()}\n",
    "                        dicom_dict['File'] = file\n",
    "\n",
    "                        # Populate metadata_dicts with unique values for each parameter, skipping excluded parameters\n",
    "                        for key, value in dicom_dict.items():\n",
    "                            if key in exclude_parameters:\n",
    "                                continue\n",
    "\n",
    "                            # Convert lists, sequences, and datasets recursively\n",
    "                            value = convert_to_tuple(value)\n",
    "\n",
    "                            metadata_dicts[key].add(value)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                    # Increase count and trigger garbage collection when batch size is reached\n",
    "                    file_count += 1\n",
    "                    if file_count >= batch_size:\n",
    "                        gc.collect()\n",
    "                        file_count = 0\n",
    "\n",
    "        # Write each parameter and its unique values to the output file\n",
    "        for key, values in metadata_dicts.items():\n",
    "            writer.writerow([key, \",\".join(map(str, values))])\n",
    "\n",
    "    print(f\"Aggregated unique DICOM metadata saved to: {aggregated_metadata_file}\")\n",
    "    logging.info(f\"Aggregated unique DICOM metadata saved to: {aggregated_metadata_file}\")\n",
    "\n",
    "    # Check and update the project report file\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=['Module', 'Summary', 'Timestamp'])\n",
    "                writer.writerow({\n",
    "                    'Module': 'Remove PHI',\n",
    "                    'Summary': f\"Executed Remove PHI in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "\n",
    "# def convert_to_tuple(data):\n",
    "#     \"\"\"Convert lists, sequences, datasets, and multivalued types to tuples recursively.\"\"\"\n",
    "#     if isinstance(data, Sequence) or isinstance(data, list) or isinstance(data, MultiValue):\n",
    "#         return tuple(convert_to_tuple(item) for item in data)\n",
    "#     elif isinstance(data, Dataset):\n",
    "#         # Convert dataset to a dictionary and process recursively\n",
    "#         data_dict = {tag: convert_to_tuple(data.get(tag)) for tag in data.dir()}\n",
    "#         return tuple(data_dict.items())\n",
    "#     elif isinstance(data, dict):\n",
    "#         return {key: convert_to_tuple(value) for key, value in data.items()}\n",
    "#     return data\n",
    "\n",
    "# # Main function to aggregate DICOM metadata\n",
    "# def aggregate_dicom_metadata(directory, output_path, location='local', project_report_file=None):\n",
    "#     \"\"\"\n",
    "#     Aggregate unique DICOM metadata from the source directory and save it to a CSV file.\n",
    "    \n",
    "#     Args:\n",
    "#     - directory (str): The root directory containing DICOM files.\n",
    "#     - output_path (str): The path to save the aggregated metadata CSV file.\n",
    "#     - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "#     - project_report_file (str): The path to the project report file.\n",
    "    \n",
    "#     Returns:\n",
    "#     - None\n",
    "#     \"\"\"\n",
    "#     # Setup logging\n",
    "#     log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "#     logging.basicConfig(filename=log_file, level=logging.INFO,\n",
    "#                         format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "#     logging.info(f\"Aggregating DICOM metadata from {location} directory: {directory}\")\n",
    "\n",
    "#     # List of metadata parameters to exclude\n",
    "#     exclude_parameters = [\n",
    "#         \"AccessionNumber\", \"AcquisitionDate\", \"AcquisitionDateTime\", \"AcquisitionTime\",\n",
    "#         \"Columns\", \"ContentTime\", \"ContentDate\", \"BluePaletteColorLookupTableData\",\n",
    "#         \"AcquisitionMatrix\", \"CineRate\"\n",
    "#     ]\n",
    "\n",
    "#     # Open the CSV file for writing\n",
    "#     aggregated_metadata_file = os.path.join(output_path, \"aggregated_dicom_metadata.csv\")\n",
    "#     with open(aggregated_metadata_file, \"w\", newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "\n",
    "#         # Iterate over all DICOM files in the directory\n",
    "#         for root, dirs, files in os.walk(directory):\n",
    "#             for file in files:\n",
    "#                 if file.endswith(\".dcm\"):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     try:\n",
    "#                         dicom_data = pydicom.dcmread(file_path)\n",
    "\n",
    "#                         # Convert DICOM metadata into a dictionary\n",
    "#                         dicom_dict = {tag: dicom_data.get(tag) for tag in dicom_data.dir()}\n",
    "#                         dicom_dict['File'] = file\n",
    "\n",
    "#                         # Temporarily output metadata dictionary for inspection\n",
    "#                         #print(f\"Metadata for {file_path}: {dicom_dict}\")\n",
    "\n",
    "#                         # Process metadata and write directly to the CSV\n",
    "#                         for key, value in dicom_dict.items():\n",
    "#                             if key in exclude_parameters:\n",
    "#                                 continue\n",
    "\n",
    "#                             # Convert lists, sequences, datasets recursively\n",
    "#                             value = convert_to_tuple(value)\n",
    "\n",
    "#                             # Write the entry directly to the CSV\n",
    "#                             writer.writerow([key, \",\".join(map(str, value))])\n",
    "#                     except Exception as e:\n",
    "#                         logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "#                         continue\n",
    "\n",
    "#             # Trigger garbage collection after processing a batch\n",
    "#             gc.collect()\n",
    "\n",
    "#     print(f\"Aggregated unique DICOM metadata saved to: {aggregated_metadata_file}\")\n",
    "#     logging.info(f\"Aggregated unique DICOM metadata saved to: {aggregated_metadata_file}\")\n",
    "\n",
    "#     # Check and update the project report file\n",
    "#     if project_report_file:\n",
    "#         try:\n",
    "#             with open(project_report_file, 'a', newline='') as csvfile:\n",
    "#                 writer = csv.DictWriter(csvfile, fieldnames=['Module', 'Summary', 'Timestamp'])\n",
    "#                 writer.writerow({\n",
    "#                     'Module': 'Remove PHI',\n",
    "#                     'Summary': f\"Executed Remove PHI in {directory}\",\n",
    "#                     'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#                 })\n",
    "#         except PermissionError as e:\n",
    "#             print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "\n",
    "            \n",
    "def remove_phi(directory, output_path, values_to_remove, location='local', project_report_file=None):\n",
    "    \"\"\"\n",
    "    Remove specified values from DICOM metadata and save the modified files.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): The root directory containing DICOM files.\n",
    "    - output_path (str): The path to save the processed DICOM files or logs.\n",
    "    - values_to_remove (list[str]): A list of strings to remove from DICOM metadata.\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(f\"Processing DICOM metadata from {location} directory: {directory}\")\n",
    "\n",
    "    # Iterate over all DICOM files in the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    dicom_data = pydicom.dcmread(file_path)\n",
    "                    \n",
    "                    # Remove specified values from metadata\n",
    "                    tags_to_delete = []\n",
    "\n",
    "                    for tag in dicom_data.dir():\n",
    "                        value = dicom_data.get(tag)\n",
    "                        if isinstance(value, str) and any(v in value for v in values_to_remove):\n",
    "                            tags_to_delete.append(tag)\n",
    "\n",
    "                    # Delete specified tags from the dataset\n",
    "                    for tag in tags_to_delete:\n",
    "                        delattr(dicom_data, tag)\n",
    "\n",
    "                    # Save the modified DICOM file\n",
    "                    new_file_path = os.path.join(root, f\"modified_{file}\")\n",
    "                    dicom_data.save_as(new_file_path)\n",
    "\n",
    "                    logging.info(f\"Processed and saved modified DICOM file to: {new_file_path}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Processed DICOM files with specified PHI removed.\")\n",
    "    logging.info(f\"Processed DICOM files with specified PHI removed.\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Remove PHI',\n",
    "                    'Summary': f\"Removed specified values from DICOM metadata in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")            \n",
    "\n",
    "# Function 6: Generate summary of DICOM tags\n",
    "def generate_summary(directory, output_path, project_report_file=None):\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.info(f\"Generating summary of DICOM tags in {directory}\")\n",
    "\n",
    "    dicom_metadata = []\n",
    "    if is_s3_path(directory):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name, prefix = parse_s3_path(directory)\n",
    "        try:\n",
    "            paginator = s3.get_paginator('list_objects_v2')\n",
    "            page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "            for page in page_iterator:\n",
    "                for obj in page['Contents']:\n",
    "                    file_key = obj['Key']\n",
    "                    if file_key.endswith('.dcm'):\n",
    "                        obj_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "                        try:\n",
    "                            obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "                            dicom_data = pydicom.dcmread(obj['Body'])\n",
    "                            metadata = {\n",
    "                                \"File\": obj_path,\n",
    "                                \"PatientID\": dicom_data.get(\"PatientID\", \"\"),\n",
    "                                \"PatientName\": dicom_data.get(\"PatientName\", \"\"),\n",
    "                                \"StudyDate\": dicom_data.get(\"StudyDate\", \"\")\n",
    "                            }\n",
    "                            dicom_metadata.append(metadata)\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error processing DICOM file {obj_path}: {str(e)}\")\n",
    "        except NoCredentialsError as e:\n",
    "            logging.error(f\"Credentials error accessing S3: {str(e)}\")\n",
    "            print(f\"Credentials error: {str(e)}\")\n",
    "            return\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        dicom_data = pydicom.dcmread(file_path)\n",
    "                        metadata = {\n",
    "                            \"File\": file_path,\n",
    "                            \"PatientID\": dicom_data.get(\"PatientID\", \"\"),\n",
    "                            \"PatientName\": dicom_data.get(\"PatientName\", \"\"),\n",
    "                            \"StudyDate\": dicom_data.get(\"StudyDate\", \"\")\n",
    "                        }\n",
    "                        dicom_metadata.append(metadata)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(dicom_metadata)\n",
    "    output_file = \"dicom_summary.csv\"\n",
    "    output_file_path = os.path.join(output_path, output_file)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    logging.info(f\"Summary of DICOM tags saved to {output_file_path}\")\n",
    "    print(f\"Summary of DICOM tags saved to {output_file_path}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Generate Summary',\n",
    "                    'Summary': f\"Executed Generate Summary: Generating summary of DICOM tags in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing to project report file {project_report_file}: {str(e)}\")\n",
    "\n",
    "    return output_file_path\n",
    "\n",
    "\n",
    "# Function 7: Rename DICOM files\n",
    "def rename_dicom_files(input_directory, output_path, project_report_file=None):\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.info(f\"Renaming DICOM files in {input_directory}\")\n",
    "    \n",
    "    rename_log = []\n",
    "\n",
    "    if is_s3_path(input_directory):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name, prefix = parse_s3_path(input_directory)\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "        \n",
    "        for page in page_iterator:\n",
    "            for obj in page['Contents']:\n",
    "                file_key = obj['Key']\n",
    "                if file_key.endswith('.dcm'):\n",
    "                    try:\n",
    "                        obj_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "                        obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "                        dicom_data = pydicom.dcmread(obj['Body'])\n",
    "\n",
    "                        sop_instance_uid = dicom_data.SOPInstanceUID\n",
    "                        patient_age = dicom_data.PatientAge if 'PatientAge' in dicom_data and int(dicom_data.PatientAge) <= 89 else ''\n",
    "                        new_filename = f\"{sop_instance_uid}_{patient_age}.dcm\"\n",
    "                        new_file_key = os.path.join(os.path.dirname(file_key), new_filename)\n",
    "                        \n",
    "                        s3.copy_object(Bucket=bucket_name, CopySource={'Bucket': bucket_name, 'Key': file_key}, Key=new_file_key)\n",
    "                        s3.delete_object(Bucket=bucket_name, Key=file_key)\n",
    "                        \n",
    "                        new_file_path = f\"s3://{bucket_name}/{new_file_key}\"\n",
    "                        rename_log.append((obj_path, new_file_path))\n",
    "                        logging.info(f\"Renamed {obj_path} to {new_file_path}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error renaming DICOM file {obj_path}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(input_directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        dicom_data = pydicom.dcmread(file_path)\n",
    "                        sop_instance_uid = dicom_data.SOPInstanceUID\n",
    "                        patient_age = dicom_data.PatientAge if 'PatientAge' in dicom_data and int(dicom_data.PatientAge) <= 89 else ''\n",
    "                        new_filename = f\"{sop_instance_uid}_{patient_age}.dcm\"\n",
    "                        new_file_path = os.path.join(root, new_filename)\n",
    "                        os.rename(file_path, new_file_path)\n",
    "                        rename_log.append((file_path, new_file_path))\n",
    "                        logging.info(f\"Renamed {file_path} to {new_file_path}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error renaming DICOM file {file_path}: {str(e)}\")\n",
    "                        continue\n",
    "    \n",
    "    csv_file_path = os.path.join(output_path, 'dicom_rename_log.csv')\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Previous Name', 'New Name'])\n",
    "        for old_name, new_name in rename_log:\n",
    "            writer.writerow([os.path.basename(old_name), os.path.basename(new_name)])\n",
    "\n",
    "    logging.info(f\"Rename log saved to {csv_file_path}\")\n",
    "    print(f\"Rename log saved to {csv_file_path}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Rename DICOM Files',\n",
    "                    'Summary': f\"Executed Rename DICOM Files in {input_directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            logging.error(f\"PermissionError when writing to project report file {project_report_file}: {str(e)}\")\n",
    "\n",
    "    return csv_file_path\n",
    "\n",
    "\n",
    "# Function 8: Generate DICOM metadata Extract CSV file\n",
    "def generate_DME(directory, output_path, location='local', project_report_file=None):\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.info(f\"Generating DICOM metadata CSV file from {location} directory: {directory}\")\n",
    "\n",
    "    metadata_list = []\n",
    "\n",
    "    if is_s3_path(directory):\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name, prefix = parse_s3_path(directory)\n",
    "        try:\n",
    "            paginator = s3.get_paginator('list_objects_v2')\n",
    "            pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "            files = [obj['Key'] for page in pages for obj in page['Contents'] if obj['Key'].endswith('.dcm')]\n",
    "            total_files = len(files)\n",
    "        except NoCredentialsError as e:\n",
    "            logging.error(f\"Credentials error accessing S3: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        progress_bar = tqdm(total=total_files, desc='Processing DICOM files', unit='files')\n",
    "        for file_key in files:\n",
    "            try:\n",
    "                obj_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "                obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "                dicom_data = pydicom.dcmread(obj['Body'])\n",
    "                metadata = extract_metadata(dicom_data, obj_path)\n",
    "                metadata_list.append(metadata)\n",
    "                progress_bar.update(1)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing DICOM file {obj_path}: {str(e)}\")\n",
    "\n",
    "    else:\n",
    "        total_files = sum(len(files) for _, _, files in os.walk(directory))\n",
    "        progress_bar = tqdm(total=total_files, desc='Processing DICOM files', unit='files')\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        dicom_data = pydicom.dcmread(file_path)\n",
    "                        metadata = extract_metadata(dicom_data, file_path)\n",
    "                        metadata_list.append(metadata)\n",
    "                        progress_bar.update(1)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    metadata_df = pd.DataFrame(metadata_list)\n",
    "    output_file = os.path.join(output_path, \"DME.csv\")\n",
    "    metadata_df.to_csv(output_file, index=False)\n",
    "\n",
    "    logging.info(f\"Generated DICOM metadata CSV file: {output_file}\")\n",
    "    print(f\"Generated DICOM metadata CSV file: {output_file}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Generate DME',\n",
    "                    'Summary': f\"Executed Generate DME in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "\n",
    "    return output_file\n",
    "\n",
    "def extract_metadata(dicom_data, file_path):\n",
    "    return {\n",
    "        \"file_name\": os.path.basename(file_path),\n",
    "        \"accession_number\": str(dicom_data.get(\"AccessionNumber\", \"\")),\n",
    "        \"acquisition_type\": str(dicom_data.get(\"AcquisitionType\", \"\")),\n",
    "        \"body_part_examined\": str(dicom_data.get(\"BodyPartExamined\", \"\")),\n",
    "        \"case_ids\": str(dicom_data.get(\"PatientID\", \"\")),\n",
    "        \"contrast_bolus_agent\": str(dicom_data.get(\"ContrastBolusAgent\", \"\")),\n",
    "        \"patient_position\": str(dicom_data.get(\"PatientPosition\", \"\")),\n",
    "        \"convolution_kernel\": \"_\".join(dicom_data.get(\"ConvolutionKernel\", []) if dicom_data.get(\"ConvolutionKernel\") else \"\"),\n",
    "        \"detector_type\": str(dicom_data.get(\"DetectorType\", \"\")),\n",
    "        \"exposure_modulation_type\": str(dicom_data.get(\"ExposureModulationType\", \"\")),\n",
    "        \"image_type\": \"_\".join(dicom_data.get(\"ImageType\", []) if dicom_data.get(\"ImageType\") else \"\"),\n",
    "        \"imager_pixel_spacing\": str(dicom_data.get(\"ImagerPixelSpacing\", \"\")),\n",
    "        \"lossy_image_compression\": str(dicom_data.get(\"LossyImageCompression\", \"\")),\n",
    "        \"manufacturer\": str(dicom_data.get(\"Manufacturer\", \"\")),\n",
    "        \"manufacturer_model_name\": str(dicom_data.get(\"ManufacturerModelName\", \"\")),\n",
    "        \"modality\": str(dicom_data.get(\"Modality\", \"\")),\n",
    "        \"sop_instance_uid\": str(dicom_data.get(\"SOPInstanceUID\", \"\")),\n",
    "        \"pixel_spacing\": str(dicom_data.get(\"PixelSpacing\", \"\")),\n",
    "        \"series_description\": str(dicom_data.get(\"SeriesDescription\", \"\")),\n",
    "        \"series_uid\": str(dicom_data.get(\"SeriesInstanceUID\", \"\")),\n",
    "        \"slice_thickness\": str(dicom_data.get(\"SliceThickness\", \"\")),\n",
    "        \"spacing_between_slices\": str(dicom_data.get(\"SpacingBetweenSlices\", \"\")),\n",
    "        \"spatial_resolution\": str(dicom_data.get(\"SpatialResolution\", \"\")),\n",
    "        \"study_description\": str(dicom_data.get(\"StudyDescription\", \"\")),\n",
    "        \"study_uid\": str(dicom_data.get(\"StudyInstanceUID\", \"\")),\n",
    "        \"view_position\": str(dicom_data.get(\"ViewPosition\", \"\")),\n",
    "        \"study_date\": str(dicom_data.get(\"StudyDate\", \"\"))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to execute individual modules\n",
    "def execute_module(module_number, directory, output_path, project_report_file):\n",
    "    \"\"\"\n",
    "    Execute a specific DICOM processing module.\n",
    "    \n",
    "    Args:\n",
    "    - module_number (int): The number corresponding to the module to execute (1-8).\n",
    "    - directory (str): The root directory containing DICOM files.\n",
    "    - output_path (str): The path to save the output files.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    module_names = {\n",
    "        1: \"Verify DICOM Files\",\n",
    "        2: \"Check Duplicate SOP Instance UIDs\",\n",
    "        3: \"Check DICOM Consistency\",\n",
    "        4: \"Verify DICOM IOD Data Consistency\",\n",
    "        5: \"Remove PHI\",\n",
    "        6: \"Generate Summary\",\n",
    "        7: \"Rename DICOM Files\",\n",
    "        8: \"Generate DME\"\n",
    "    }\n",
    "    \n",
    "    if module_number in module_names:\n",
    "        print(f\"Executing Module {module_number}: {module_names[module_number]}\")\n",
    "        # Replace these function calls with actual function definitions or placeholders as required\n",
    "        function_map = {\n",
    "            1: verify_dicom_files,\n",
    "            2: check_duplicate_sop_uids,\n",
    "            3: check_dicom_consistency,\n",
    "            4: verify_dicom_iod_data,\n",
    "            5: aggregate_dicom_metadata,  # Assuming removing PHI requires aggregation of metadata\n",
    "            6: generate_summary,\n",
    "            7: rename_dicom_files,\n",
    "            8: generate_DME\n",
    "        }\n",
    "        func = function_map.get(module_number)\n",
    "        if func:\n",
    "            func(directory, output_path, project_report_file)\n",
    "            print(\"Done\")\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter a number between 1 and 8.\")\n",
    "\n",
    "# Function to execute all modules with option to skip\n",
    "def execute_all_modules(source_path, output_path, project_report_file):\n",
    "    print(\"Executing all functions in succession with option to skip each step:\")\n",
    "    for i in range(1, 9):\n",
    "        choice = input(f\"Execute module {i}  {project_report_file}? (yes/no): \")\n",
    "        if choice.lower() == 'yes':\n",
    "            execute_module(i, source_path, output_path, project_report_file)\n",
    "        else:\n",
    "            print(f\"Skipping module {i}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name=None, project_name=None, log_date=None, output_path=None, source_path=None):\n",
    "    \"\"\"\n",
    "    Main function to execute the DICOM processing tool.\n",
    "\n",
    "    Args:\n",
    "    - name (str): User's name.\n",
    "    - project_name (str): Name of the project.\n",
    "    - log_date (str): Date for the log file (YYYY-MM-DD).\n",
    "    - output_path (str): Output directory path.\n",
    "    - source_path (str): Path to the source DICOM files.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if not all([name, project_name, log_date, output_path, source_path]):\n",
    "        name, project_name, log_date, output_path, source_path = prompt_user()\n",
    "\n",
    "    log_file_name = f\"{project_name}_log_{log_date}.csv\"\n",
    "    log_file_path = os.path.join(output_path, log_file_name)\n",
    "\n",
    "    create_log_file(log_file_path)\n",
    "\n",
    "    project_report_name = f\"{project_name}_report_{log_date}.csv\"\n",
    "    project_report_file = os.path.join(output_path, project_report_name)\n",
    "\n",
    "    print(f\"Hello, {name}! Welcome to {project_name} DICOM Processing Tool.\")\n",
    "    print(\"Choose an option:\")\n",
    "    print(\"1. Execute individual modules\")\n",
    "    print(\"2. Execute all functions in succession with option to skip\")\n",
    "    option = int(input(\"Enter your choice (1-2): \"))\n",
    "\n",
    "    if option == 1:\n",
    "        module_number = int(input(\"Enter the module number you want to execute (1-8): \"))\n",
    "        execute_module(module_number, source_path, output_path, project_report_file)\n",
    "    elif option == 2:\n",
    "        execute_all_modules(source_path, output_path, project_report_file)\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
    "\n",
    "# Additional utility functions used in the above code might include:\n",
    "# - verify_dicom_files\n",
    "# - check_duplicate_sop_uids\n",
    "# - check_dicom_consistency\n",
    "# - verify_dicom_iod_data\n",
    "# - aggregate_dicom_metadata\n",
    "# - generate_summary\n",
    "# - rename_dicom_files\n",
    "# - generate_DME\n",
    "# - create_log_file\n",
    "# - prompt_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, ToS! Welcome to UCSF56 DICOM Processing Tool.\n",
      "Choose an option:\n",
      "1. Execute individual modules\n",
      "2. Execute all functions in succession with option to skip\n",
      "Executing Module 8: Generate DME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DICOM files:  61%|██████    | 67220/110711 [11:24<03:51, 187.90files/s] c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (19) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (24) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (42) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (18) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (43) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "Processing DICOM files: 100%|██████████| 110711/110711 [19:05<00:00, 96.66files/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated DICOM metadata CSV file: C:/Users/tosullivan/Documents/Dev/Test_output\\DME.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "   name = \"Name\"\n",
    "   project_name = \"Name\"\n",
    "   log_date = \"YYYY-MM-DD\"\n",
    "   output_path = \"path here\"\n",
    "   source_path = \"path here\"\n",
    "   main(name, project_name, log_date, output_path, source_path)\n",
    "   #main()\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
