{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (3.8.4)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (8.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: boto3 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (1.34.95)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.95 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from boto3) (1.34.95)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from botocore<1.35.0,>=1.34.95->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from botocore<1.35.0,>=1.34.95->boto3) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tosullivan\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.95->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install matplotlib ipywidgets\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import gc\n",
    "import pydicom\n",
    "from pydicom.datadict import dictionary_VR\n",
    "from pydicom.sequence import Sequence\n",
    "from pydicom.dataset import Dataset\n",
    "from pydicom.multival import MultiValue\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import botocore\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create log file\n",
    "def create_log_file(log_file_name):\n",
    "    logging.basicConfig(filename=log_file_name, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to prompt user for information\n",
    "def prompt_user():\n",
    "    name = input(\"Enter your name: \")\n",
    "    project_name = input(\"Enter project name: \")\n",
    "    log_date = input(\"Enter log file date (YYYY-MM-DD): \")\n",
    "    output_path = input(\"Enter the output directory path: \")\n",
    "    source_path = input(\"Enter the source directory path (local or S3 bucket): \")\n",
    "    return name, project_name, log_date, output_path, source_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append to project report CSV file\n",
    "def append_to_project_report(module_name, summary, output_path, user_name):\n",
    "    report_file = os.path.join(output_path, \"project_report.csv\")\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(report_file, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([now, user_name, module_name, summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions from the second notebook\n",
    "\n",
    "# AWS S3 Setup\n",
    "s3 = boto3.client('s3')\n",
    "# Function 1: Verify DICOM files\n",
    "def verify_dicom_files(directory, output_path, project_report_file=None):\n",
    "    \"\"\"\n",
    "    Verify DICOM files in a directory (local or S3 bucket).\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory to search for DICOM files.\n",
    "    - output_path (str): The path to save the output CSV file and log file.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(f\"Verifying DICOM files in directory: {directory}\")\n",
    "    \n",
    "    # Create an empty list to store verification results\n",
    "    verification_results = []\n",
    "    \n",
    "    # Iterate over all DICOM files in the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Verify DICOM file\n",
    "                    dicom_data = pydicom.dcmread(file_path)\n",
    "                    \n",
    "                    # Add verification result to the list\n",
    "                    verification_results.append({\n",
    "                        \"File\": file_path,\n",
    "                        \"Verification\": \"Passed\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error verifying DICOM file {file_path}: {str(e)}\")\n",
    "                    verification_results.append({\n",
    "                        \"File\": file_path,\n",
    "                        \"Verification\": \"Failed\"\n",
    "                    })\n",
    "                    continue\n",
    "    \n",
    "    # Convert the list of verification results to a DataFrame\n",
    "    verification_df = pd.DataFrame(verification_results)\n",
    "    \n",
    "    # Save verification results to a CSV file\n",
    "    verification_csv_file = os.path.join(output_path, \"dicom_verification_results.csv\")\n",
    "    verification_df.to_csv(verification_csv_file, index=False)\n",
    "    \n",
    "    logging.info(f\"Verification results saved to {verification_csv_file}\")\n",
    "    print(f\"Verification results saved to {verification_csv_file}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Verify DICOM Files',\n",
    "                    'Summary': f\"Executed Verify DICOM Files {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "\n",
    "\n",
    "# Function 2: Check for duplicate SOP Instance UIDs\n",
    "def check_duplicate_sop_uids(directory, output_path, project_report_file=None):\n",
    "    \"\"\"\n",
    "    Check for duplicate SOP Instance UIDs in DICOM files within a directory (local or S3 bucket).\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory to search for DICOM files.\n",
    "    - output_path (str): The path to save the output CSV file and log file.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(f\"Checking for duplicate SOP Instance UIDs in directory: {directory}\")\n",
    "    \n",
    "    # Create a defaultdict to store lists of files with duplicate SOP UID for each UID\n",
    "    duplicate_uids = defaultdict(list)\n",
    "    \n",
    "    # Create a dictionary to store duplicate SOPs and corresponding paths\n",
    "    duplicates_dict = {}\n",
    "    \n",
    "    # Iterate over all DICOM files in the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Read DICOM file and extract SOP Instance UID\n",
    "                    dicom_data = pydicom.dcmread(file_path)\n",
    "                    sop_instance_uid = dicom_data.SOPInstanceUID\n",
    "                    \n",
    "                    # Check if SOP Instance UID already exists in the defaultdict\n",
    "                    if sop_instance_uid in duplicate_uids:\n",
    "                        # If exists, add file path to the list\n",
    "                        duplicate_uids[sop_instance_uid].append(file_path)\n",
    "                        # Add to the duplicates dictionary\n",
    "                        duplicates_dict.setdefault(sop_instance_uid, []).append(file_path)\n",
    "                    else:\n",
    "                        # If not exists, create a new list with the file path\n",
    "                        duplicate_uids[sop_instance_uid] = [file_path]\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Filter the dictionary to include only duplicates\n",
    "    duplicates_dict = {key: value for key, value in duplicates_dict.items() if len(value) > 1}\n",
    "    \n",
    "    # Save duplicate SOP Instance UIDs and corresponding paths to a CSV file\n",
    "    duplicate_uids_csv_file = os.path.join(output_path, \"duplicate_sop_instance_uids.csv\")\n",
    "    with open(duplicate_uids_csv_file, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"SOPInstanceUID\", \"FilePaths\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for uid, file_paths in duplicates_dict.items():\n",
    "            writer.writerow({\"SOPInstanceUID\": uid, \"FilePaths\": \", \".join(file_paths)})\n",
    "    \n",
    "    logging.info(f\"Duplicate SOP Instance UIDs and corresponding paths saved to {duplicate_uids_csv_file}\")\n",
    "    print(f\"Duplicate SOP Instance UIDs and corresponding paths saved to {duplicate_uids_csv_file}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Check Duplicate SOP Instance UIDs',\n",
    "                    'Summary': f\"Executed Check Duplicate SOP Instance UIDs {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Function 3: Check DICOM consistency\n",
    "def check_dicom_consistency(directory, output_path, project_report_file=None):\n",
    "    \"\"\"\n",
    "    Perform basic and extended consistency checks for a directory containing folders of DICOM files.\n",
    "\n",
    "    Args:\n",
    "    - directory: Path to the directory containing folders of DICOM files.\n",
    "    - output_path: Path to save the output CSV file.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "\n",
    "    Returns:\n",
    "    - csv_file_path: Path to the output CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store errors\n",
    "    errors = defaultdict(list)\n",
    "\n",
    "    # Iterate over each folder (DICOM series) in the directory\n",
    "    for series_folder in os.listdir(directory):\n",
    "        series_path = os.path.join(directory, series_folder)\n",
    "\n",
    "        if not os.path.isdir(series_path):\n",
    "            continue\n",
    "\n",
    "        # Collect DICOM files within the series folder\n",
    "        dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n",
    "\n",
    "        # Check if there are DICOM files in the series folder\n",
    "        if not dicom_files:\n",
    "            errors[series_folder].append((None, \"No DICOM files found in this series folder.\"))\n",
    "            continue\n",
    "\n",
    "        # Initialize variables to store attributes for consistency checks\n",
    "        attributes = defaultdict(list)\n",
    "        image_positions = []\n",
    "\n",
    "        # Iterate over DICOM files in the series folder\n",
    "        for dicom_file in tqdm(dicom_files, desc=f'Processing {series_folder}'):\n",
    "            file_path = os.path.join(series_path, dicom_file)\n",
    "\n",
    "            try:\n",
    "                # Read DICOM file\n",
    "                dicom_data = pydicom.dcmread(file_path)\n",
    "\n",
    "                # Check consistency of essential attributes\n",
    "                essential_attributes = ['PatientID', 'StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID']\n",
    "                for attribute in essential_attributes:\n",
    "                    if attribute not in dicom_data:\n",
    "                        errors[series_folder].append((file_path, f\"Missing {attribute} in DICOM file: {dicom_file}\"))\n",
    "                    else:\n",
    "                        attributes[attribute].append(dicom_data[attribute].value)\n",
    "\n",
    "                # Additional consistency checks\n",
    "                if 'StudyDate' in dicom_data:\n",
    "                    attributes['StudyDate'].append(dicom_data.StudyDate)\n",
    "                if 'StudyTime' in dicom_data:\n",
    "                    attributes['StudyTime'].append(dicom_data.StudyTime)\n",
    "                if 'SeriesDate' in dicom_data:\n",
    "                    attributes['SeriesDate'].append(dicom_data.SeriesDate)\n",
    "                if 'SeriesTime' in dicom_data:\n",
    "                    attributes['SeriesTime'].append(dicom_data.SeriesTime)\n",
    "                if 'Modality' in dicom_data:\n",
    "                    attributes['Modality'].append(dicom_data.Modality)\n",
    "                if 'ImagePositionPatient' in dicom_data:\n",
    "                    image_positions.append(dicom_data.ImagePositionPatient)\n",
    "                if 'ImageOrientationPatient' in dicom_data:\n",
    "                    attributes['ImageOrientationPatient'].append(dicom_data.ImageOrientationPatient)\n",
    "                if 'PixelSpacing' in dicom_data:\n",
    "                    attributes['PixelSpacing'].append(dicom_data.PixelSpacing)\n",
    "                if 'SOPClassUID' in dicom_data:\n",
    "                    attributes['SOPClassUID'].append(dicom_data.SOPClassUID)\n",
    "\n",
    "                # Check image consistency\n",
    "                if len(image_positions) > 1 and len(set(image_positions)) != 1:\n",
    "                    errors[series_folder].append((file_path, \"Inconsistent ImagePositionPatient values across DICOM files.\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                errors[series_folder].append((file_path, f\"Error processing DICOM file {dicom_file}: {str(e)}\"))\n",
    "\n",
    "    # Write errors to CSV file\n",
    "    csv_file_path = os.path.join(output_path, 'dicom_consistency_errors.csv')\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['SeriesFolder', 'FilePath', 'Error'])\n",
    "        for series_folder, error_list in errors.items():\n",
    "            for error in error_list:\n",
    "                writer.writerow([series_folder, error[0], error[1]])\n",
    "\n",
    "    print(f\"Output CSV file saved to: {csv_file_path}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Check DICOM Consistency',\n",
    "                    'Summary': f\"Executed Check DICOM Consistency: Checking DICOM consistency in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "\n",
    "\n",
    "    return csv_file_path\n",
    "\n",
    "# Function 4: Verify DICOM IOD data consistency\n",
    "# Modality-specific rules\n",
    "modality_rules = {\n",
    "    'CT': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['CT']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {\n",
    "            'SliceThickness': 'required'\n",
    "        }\n",
    "    },\n",
    "    'MRI': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['MR']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {}\n",
    "    },\n",
    "    'US': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['US']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {}\n",
    "    },\n",
    "    'PET': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['PT']), 'Manufacturer': 'LO'\n",
    "        }\n",
    "    },\n",
    "    'PT': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['PT']), 'Manufacturer': 'LO',  \n",
    "        },\n",
    "        'conditional': {\n",
    "            'FrameReferenceTime': 'required'\n",
    "        }\n",
    "    },\n",
    "    'DX': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['DX']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {\n",
    "            'Exposure': 'required'\n",
    "        }\n",
    "    },\n",
    "    'MR': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['MR']), 'Manufacturer': 'LO',\n",
    "            'MagneticFieldStrength': 'DS',  \n",
    "            'EchoTime': 'DS',  \n",
    "            'RepetitionTime': 'DS' \n",
    "        },\n",
    "        'conditional': {\n",
    "            'FlipAngle': 'required'\n",
    "        }\n",
    "    },\n",
    "    'RF': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['RF']), 'Manufacturer': 'LO',\n",
    "        },\n",
    "        'conditional': {\n",
    "            'FrameTime': 'required'\n",
    "        }\n",
    "    },\n",
    "    'XA': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['XA']), 'Manufacturer': 'LO',\n",
    "        },\n",
    "        'conditional': {\n",
    "            'ExposureTime': 'required'\n",
    "        }\n",
    "    },\n",
    "    'CR': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['CR']), 'Manufacturer': 'LO',\n",
    "        },\n",
    "        'conditional': {\n",
    "            'Exposure': 'required'\n",
    "        }\n",
    "    },\n",
    "    'NM': {\n",
    "        'required': {\n",
    "            'PatientID': 'UI', 'StudyInstanceUID': 'UI', 'SeriesInstanceUID': 'UI',\n",
    "            'SOPInstanceUID': 'UI', 'Modality': ('CS', ['NM']), 'Manufacturer': 'LO'\n",
    "        },\n",
    "        'conditional': {\n",
    "            'FrameReferenceTime': 'required'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def validate_dicom_modality(ds, modality):\n",
    "    errors = []\n",
    "\n",
    "    # Check if the modality is recognized and in the dictionary\n",
    "    if modality not in modality_rules:\n",
    "        errors.append(f\"Unsupported or undefined modality: {modality}\")\n",
    "        return errors  # Return early as further validation doesn't make sense\n",
    "\n",
    "    # Get the rules associated with the modality\n",
    "    rules = modality_rules[modality]\n",
    "\n",
    "    for attr, attr_type in rules['required'].items():\n",
    "        if not hasattr(ds, attr):\n",
    "            if attr_type == \"UI\":\n",
    "                continue  # Silently skip attributes expected to be \"UI\"\n",
    "            errors.append(f\"Missing attribute: {attr}\")\n",
    "        else:\n",
    "            actual_value = getattr(ds, attr)\n",
    "\n",
    "            # Directly handle simple types\n",
    "            if isinstance(actual_value, str):\n",
    "                if isinstance(attr_type, tuple):\n",
    "                    expected_type, expected_values = attr_type\n",
    "                    if expected_type != \"LO\" and actual_value not in expected_values:\n",
    "                        errors.append(f\"Attribute {attr} has an incorrect value: {actual_value} not in {expected_values}\")\n",
    "                elif attr_type != \"LO\":\n",
    "                    if attr_type == \"UI\":\n",
    "                        continue  # Skip logging for this type mismatch\n",
    "                    errors.append(f\"Attribute {attr} has incorrect type: expected {attr_type}, got str\")\n",
    "            elif hasattr(actual_value, 'VR'):  # Complex DICOM element case\n",
    "                if actual_value.VR != attr_type:\n",
    "                    if attr_type == \"UI\":\n",
    "                        continue  # Silently skip this type mismatch\n",
    "                    errors.append(f\"Attribute {attr} has incorrect type: expected {attr_type}, got {actual_value.VR}\")\n",
    "                elif isinstance(attr_type, tuple):\n",
    "                    expected_values = attr_type[1]\n",
    "                    if actual_value.value not in expected_values:\n",
    "                        errors.append(f\"Attribute {attr} has an incorrect value: {actual_value.value} not in {expected_values}\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "def verify_dicom_iod_data(directory, output_path, location='local', project_report_file=None):\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    logging.info(f\"Verifying DICOM IOD data consistency in {location} directory: {directory}\")\n",
    "\n",
    "    iod_verification_report = {}\n",
    "\n",
    "    if location == 's3':\n",
    "        # Process files from an S3 bucket\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=directory):\n",
    "            for item in tqdm(page.get('Contents', [])):\n",
    "                file_key = item['Key']\n",
    "                if file_key.endswith('.dcm'):\n",
    "                    try:\n",
    "                        obj = s3.get_object(Bucket=directory, Key=file_key)\n",
    "                        dicom_data = pydicom.dcmread(BytesIO(obj['Body'].read()))\n",
    "\n",
    "                        modality = dicom_data.Modality if 'Modality' in dicom_data else None\n",
    "                        if modality:\n",
    "                            print(f\"Modality found: {modality}\")\n",
    "                        else:\n",
    "                            print(\"No Modality attribute found\")\n",
    "\n",
    "                        if \"PixelData\" not in dicom_data:\n",
    "                            logging.warning(f\"Missing PixelData in DICOM file: {file_key}\")\n",
    "                            iod_verification_report[file_key] = \"Missing PixelData\"\n",
    "                        else:\n",
    "                            errors = validate_dicom_modality(dicom_data, modality)\n",
    "                            if errors:\n",
    "                                iod_verification_report[file_key] = \", \".join(errors)\n",
    "                                logging.warning(f\"Verification issues in {file_key}: {', '.join(errors)}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing DICOM file {file_key}: {str(e)}\")\n",
    "    else:\n",
    "        # Process files from a local directory\n",
    "        for root, dirs, files in tqdm(os.walk(directory)):\n",
    "            for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        dicom_data = pydicom.dcmread(file_path)\n",
    "\n",
    "                        modality = dicom_data.Modality if 'Modality' in dicom_data else None\n",
    "                        if modality is None:\n",
    "                            print(\"No Modality attribute found\")\n",
    "\n",
    "                        if \"PixelData\" not in dicom_data:\n",
    "                            logging.warning(f\"Missing PixelData in DICOM file: {file_path}\")\n",
    "                            iod_verification_report[file_path] = \"Missing PixelData\"\n",
    "                        else:\n",
    "                            errors = validate_dicom_modality(dicom_data, modality)\n",
    "                            if errors:\n",
    "                                iod_verification_report[file_path] = \", \".join(errors)\n",
    "                                logging.warning(f\"Verification issues in {file_path}: {', '.join(errors)}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "\n",
    "    # Save verification report to a CSV file\n",
    "    verification_report_csv = os.path.join(output_path, \"iod_verification_report.csv\")\n",
    "    with open(verification_report_csv, \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"File\", \"Issue\"])\n",
    "        for file_path, issue in iod_verification_report.items():\n",
    "            writer.writerow([file_path, issue])\n",
    "\n",
    "    logging.info(f\"IOD verification report saved to {verification_report_csv}\")\n",
    "    print(f\"IOD verification report saved to {verification_report_csv}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=['Module', 'Summary', 'Timestamp'])\n",
    "                writer.writerow({\n",
    "                    'Module': 'Verify DICOM IOD Data Consistency',\n",
    "                    'Summary': f\"Executed Verify DICOM IOD Data Consistency in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}\")\n",
    "\n",
    "    return iod_verification_report\n",
    "\n",
    "# Function 5: Remove PHI info from DICOM metadata\n",
    "\n",
    "def convert_to_tuple(data):\n",
    "    \n",
    "    \"\"\"Convert lists, sequences, datasets, and multivalued types to tuples recursively.\"\"\"\n",
    "    if isinstance(data, Sequence) or isinstance(data, list) or isinstance(data, MultiValue):\n",
    "        return tuple(convert_to_tuple(item) for item in data)\n",
    "    elif isinstance(data, Dataset):\n",
    "        # Convert dataset to a dictionary and process recursively\n",
    "        data_dict = {tag: convert_to_tuple(data.get(tag)) for tag in data.dir()}\n",
    "        return tuple(data_dict.items())\n",
    "    elif isinstance(data, dict):\n",
    "        return {key: convert_to_tuple(value) for key, value in data.items()}\n",
    "    return data\n",
    "\n",
    "def convert_data(data):\n",
    "    \"\"\"Convert complex data types to a simpler form.\"\"\"\n",
    "    if isinstance(data, pydicom.multival.MultiValue) or isinstance(data, list):\n",
    "        return tuple(convert_data(item) for item in data)\n",
    "    elif isinstance(data, pydicom.dataset.Dataset):\n",
    "        data_dict = {tag: convert_data(data[tag].value) for tag in data.dir()}\n",
    "        return data_dict\n",
    "    elif isinstance(data, str):\n",
    "        return data\n",
    "    elif hasattr(data, \"value\"):\n",
    "        return data.value\n",
    "    return data\n",
    "\n",
    "def aggregate_dicom_metadata(directory, output_path, batch_size=10, location='local', project_report_file=None):\n",
    "    \"\"\"\n",
    "    Aggregate unique DICOM metadata from the source directory and save it to a transposed CSV file.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): The root directory containing DICOM files.\n",
    "    - output_path (str): The path to save the aggregated metadata CSV file.\n",
    "    - batch_size (int): Number of DICOM files to process per batch.\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    logging.info(f\"Aggregating DICOM metadata from {location} directory: {directory}\")\n",
    "\n",
    "    # List of metadata parameters to exclude\n",
    "    exclude_parameters = [\n",
    "        \"AccessionNumber\", \"AcquisitionDate\", \"AcquisitionDateTime\", \"AcquisitionTime\",\n",
    "        \"Columns\", \"ContentTime\", \"ContentDate\", \"BluePaletteColorLookupTableData\",\n",
    "        \"AcquisitionMatrix\", \"CineRate\"\n",
    "    ]\n",
    "\n",
    "    # Open the output CSV file in advance\n",
    "    aggregated_metadata_file = os.path.join(output_path, \"aggregated_dicom_metadata.csv\")\n",
    "    with open(aggregated_metadata_file, \"w\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write headers once at the top\n",
    "        writer.writerow([\"Parameter\", \"Unique Values\"])\n",
    "        \n",
    "        # Initialize a defaultdict of sets to store unique values for each parameter\n",
    "        metadata_dicts = defaultdict(set)\n",
    "\n",
    "        # Iterate over all DICOM files in the directory\n",
    "        for root, dirs, files in tqdm(os.walk(directory), desc=\"Processing folders\"):\n",
    "            for file in files:\n",
    "                if file.endswith(\".dcm\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        dicom_data = pydicom.dcmread(file_path)\n",
    "\n",
    "                        # Convert DICOM metadata to a dictionary\n",
    "                        dicom_dict = {tag: dicom_data.get(tag) for tag in dicom_data.dir()}\n",
    "                        dicom_dict['File'] = file\n",
    "\n",
    "                        # Process metadata and add to the dictionary\n",
    "                        for key, value in dicom_dict.items():\n",
    "                            if key in exclude_parameters:\n",
    "                                continue\n",
    "\n",
    "                            # Convert complex data types recursively\n",
    "                            value = convert_data(value)\n",
    "                            \n",
    "                            # Convert lists to string\n",
    "                            if isinstance(value, list):\n",
    "                                value = tuple(value)\n",
    "                            metadata_dicts[key].add(value)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "        # Write metadata_dicts to the output CSV incrementally\n",
    "        for key, values in metadata_dicts.items():\n",
    "            writer.writerow([key, \",\".join(map(str, values))])\n",
    "            \n",
    "        # Trigger garbage collection after writing\n",
    "        gc.collect()\n",
    "\n",
    "    logging.info(f\"Aggregated transposed DICOM metadata saved to: {aggregated_metadata_file}\")\n",
    "    print(f\"Aggregated transposed DICOM metadata saved to: {aggregated_metadata_file}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=['Module', 'Summary', 'Timestamp'])\n",
    "                writer.writerow({\n",
    "                    'Module': 'Remove PHI',\n",
    "                    'Summary': f\"Executed Remove PHI in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}\")\n",
    "\n",
    "            \n",
    "def remove_phi(directory, output_path, values_to_remove, location='local', project_report_file=None):\n",
    "    \"\"\"\n",
    "    Remove specified values from DICOM metadata and save the modified files.\n",
    "\n",
    "    Args:\n",
    "    - directory (str): The root directory containing DICOM files.\n",
    "    - output_path (str): The path to save the processed DICOM files or logs.\n",
    "    - values_to_remove (list[str]): A list of strings to remove from DICOM metadata.\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    log_file = os.path.join(output_path, \"dicom_processing.log\")\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(f\"Processing DICOM metadata from {location} directory: {directory}\")\n",
    "\n",
    "    # Iterate over all DICOM files in the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    dicom_data = pydicom.dcmread(file_path)\n",
    "                    \n",
    "                    # Remove specified values from metadata\n",
    "                    tags_to_delete = []\n",
    "\n",
    "                    for tag in dicom_data.dir():\n",
    "                        value = dicom_data.get(tag)\n",
    "                        if isinstance(value, str) and any(v in value for v in values_to_remove):\n",
    "                            tags_to_delete.append(tag)\n",
    "\n",
    "                    # Delete specified tags from the dataset\n",
    "                    for tag in tags_to_delete:\n",
    "                        delattr(dicom_data, tag)\n",
    "\n",
    "                    # Save the modified DICOM file\n",
    "                    new_file_path = os.path.join(root, f\"modified_{file}\")\n",
    "                    dicom_data.save_as(new_file_path)\n",
    "\n",
    "                    logging.info(f\"Processed and saved modified DICOM file to: {new_file_path}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Processed DICOM files with specified PHI removed.\")\n",
    "    logging.info(f\"Processed DICOM files with specified PHI removed.\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Remove PHI',\n",
    "                    'Summary': f\"Removed specified values from DICOM metadata in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")            \n",
    "\n",
    "# Function 6: Generate summary of DICOM tags\n",
    "def generate_summary(directory, output_path, project_report_file=None):\n",
    "    logging.info(f\"Generating summary of DICOM tags in {directory}\")\n",
    "    \n",
    "    # Initialize an empty list to store DICOM metadata\n",
    "    dicom_metadata = []\n",
    "    \n",
    "    # Iterate over all DICOM files in the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    dicom_data = pydicom.dcmread(file_path)\n",
    "                    \n",
    "                    # Extract metadata from DICOM file\n",
    "                    metadata = {\n",
    "                        \"File\": file_path,\n",
    "                        \"PatientID\": dicom_data.get(\"PatientID\", \"\"),\n",
    "                        \"PatientName\": dicom_data.get(\"PatientName\", \"\"),\n",
    "                        \"StudyDate\": dicom_data.get(\"StudyDate\", \"\")\n",
    "                    }\n",
    "                    dicom_metadata.append(metadata)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Convert the list of metadata dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(dicom_metadata)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file at the specified output path\n",
    "    output_file = \"dicom_summary.csv\"\n",
    "    output_file_path = os.path.join(output_path, output_file)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    logging.info(f\"Summary of DICOM tags saved to {output_file_path}\")\n",
    "    print(f\"Summary of DICOM tags saved to {output_file_path}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Generate Summary',\n",
    "                    'Summary': f\"Executed Generate Summary: Generating summary of DICOM tags in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing to project report file {project_report_file}: {str(e)}\")\n",
    "\n",
    "\n",
    "    return output_file_path\n",
    "\n",
    "# Function 7: Rename DICOM files\n",
    "def rename_dicom_files(input_directory, output_path, project_report_file=None):\n",
    "    logging.info(f\"Renaming DICOM files in {input_directory}\")\n",
    "    \n",
    "    # Initialize a list to store rename log entries\n",
    "    rename_log = []\n",
    "    \n",
    "    # Iterate over all DICOM files in the directory\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    dicom_data = pydicom.dcmread(file_path)\n",
    "                    \n",
    "                    # Extract SOP Instance UID\n",
    "                    sop_instance_uid = dicom_data.SOPInstanceUID\n",
    "                    \n",
    "                    # Extract patient's age and check if it's empty or > 89\n",
    "                    patient_age = dicom_data.PatientAge\n",
    "                    if not patient_age or int(patient_age) > 89:\n",
    "                        patient_age = ''\n",
    "                    \n",
    "                    # Generate new filename based on SOP Instance UID and empty age\n",
    "                    new_filename = f\"{sop_instance_uid}_{patient_age}.dcm\"\n",
    "                    \n",
    "                    # Rename the DICOM file\n",
    "                    new_file_path = os.path.join(root, new_filename)\n",
    "                    os.rename(file_path, new_file_path)\n",
    "                    \n",
    "                    # Log the renaming action\n",
    "                    rename_log.append((file_path, new_file_path))\n",
    "                    logging.info(f\"Renamed {file_path} to {new_file_path}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error renaming DICOM file {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Save the rename log to a CSV file at the specified output path\n",
    "    csv_file_path = os.path.join(output_path, 'dicom_rename_log.csv')\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Previous Name', 'New Name', 'File Path'])\n",
    "        for old_name, new_name in rename_log:\n",
    "            writer.writerow([os.path.basename(old_name), os.path.basename(new_name), new_name])\n",
    "    \n",
    "    logging.info(f\"Rename log saved to {csv_file_path}\")\n",
    "    print(f\"Rename log saved to {csv_file_path}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Rename DICOM Files',\n",
    "                    'Summary': f\"Executed Rename DICOM Files in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "\n",
    "    return csv_file_path\n",
    "\n",
    "\n",
    "# Function 8: Generate DICOM metadata Extract CSV file\n",
    "def generate_DME(directory, output_path, location='local', project_report_file=None):\n",
    "    logging.info(f\"Generating DICOM metadata CSV file from {location} directory: {directory}\")\n",
    "\n",
    "    metadata_list = []\n",
    "    total_files = sum(len(files) for _, _, files in os.walk(directory))\n",
    "    progress_bar = tqdm(total=total_files, desc='Processing DICOM files', unit='files')\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    dicom_data = pydicom.dcmread(file_path)\n",
    "                    metadata = {\n",
    "                        \"file_name\": os.path.basename(file_path),\n",
    "                        \"accession_number\": str(dicom_data.get(\"AccessionNumber\", \"\")),\n",
    "                        \"acquisition_type\": str(dicom_data.get(\"AcquisitionType\", \"\")),\n",
    "                        \"body_part_examined\": str(dicom_data.get(\"BodyPartExamined\", \"\")),\n",
    "                        \"case_ids\": str(dicom_data.get(\"PatientID\", \"\")),\n",
    "                        \"contrast_bolus_agent\": str(dicom_data.get(\"ContrastBolusAgent\", \"\")),\n",
    "                        \"patient_position\": str(dicom_data.get(\"PatientPosition\", \"\")),\n",
    "                        \"convolution_kernel\": \"_\".join(dicom_data.get(\"ConvolutionKernel\", []) if dicom_data.get(\"ConvolutionKernel\") else \"\"),\n",
    "                        \"detector_type\": str(dicom_data.get(\"DetectorType\", \"\")),\n",
    "                        \"exposure_modulation_type\": str(dicom_data.get(\"ExposureModulationType\", \"\")),\n",
    "                        \"image_type\": \"_\".join(dicom_data.get(\"ImageType\", []) if dicom_data.get(\"ImageType\") else \"\"),\n",
    "                        \"imager_pixel_spacing\": str(dicom_data.get(\"ImagerPixelSpacing\", \"\")),\n",
    "                        \"lossy_image_compression\": str(dicom_data.get(\"LossyImageCompression\", \"\")),\n",
    "                        \"manufacturer\": str(dicom_data.get(\"Manufacturer\", \"\")),\n",
    "                        \"manufacturer_model_name\": str(dicom_data.get(\"ManufacturerModelName\", \"\")),\n",
    "                        \"modality\": str(dicom_data.get(\"Modality\", \"\")),\n",
    "                        \"sop_instance_uid\": str(dicom_data.get(\"SOPInstanceUID\", \"\")),\n",
    "                        \"pixel_spacing\": str(dicom_data.get(\"PixelSpacing\", \"\")),\n",
    "                        \"series_description\": str(dicom_data.get(\"SeriesDescription\", \"\")),\n",
    "                        \"series_uid\": str(dicom_data.get(\"SeriesInstanceUID\", \"\")),\n",
    "                        \"slice_thickness\": str(dicom_data.get(\"SliceThickness\", \"\")),\n",
    "                        \"spacing_between_slices\": str(dicom_data.get(\"SpacingBetweenSlices\", \"\")),\n",
    "                        \"spatial_resolution\": str(dicom_data.get(\"SpatialResolution\", \"\")),\n",
    "                        \"study_description\": str(dicom_data.get(\"StudyDescription\", \"\")),\n",
    "                        \"study_uid\": str(dicom_data.get(\"StudyInstanceUID\", \"\")),\n",
    "                        \"view_position\": str(dicom_data.get(\"ViewPosition\", \"\")),\n",
    "                        \"study_date\": str(dicom_data.get(\"StudyDate\", \"\"))\n",
    "                    }\n",
    "                    metadata_list.append(metadata)\n",
    "                    progress_bar.update(1)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing DICOM file {file_path}: {str(e)}\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    metadata_df = pd.DataFrame(metadata_list)\n",
    "    output_file = os.path.join(output_path, \"DME.csv\")\n",
    "    metadata_df.to_csv(output_file, index=False)\n",
    "\n",
    "    logging.info(f\"Generated DICOM metadata CSV file: {output_file}\")\n",
    "    print(f\"Generated DICOM metadata CSV file: {output_file}\")\n",
    "\n",
    "    if project_report_file:\n",
    "        try:\n",
    "            with open(project_report_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['Module', 'Summary', 'Timestamp']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    'Module': 'Generate DME',\n",
    "                    'Summary': f\"Executed Generate DME in {directory}\",\n",
    "                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError when writing to project report file: {e}. Please ensure it is not open in another program and that you have the necessary permissions.\")\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to execute individual modules\n",
    "def execute_module(module_number, directory, output_path, project_report_file):\n",
    "    \"\"\"\n",
    "    Execute a specific DICOM processing module.\n",
    "    \n",
    "    Args:\n",
    "    - module_number (int): The number corresponding to the module to execute (1-8).\n",
    "    - directory (str): The root directory containing DICOM files.\n",
    "    - output_path (str): The path to save the output files.\n",
    "    - project_report_file (str): The path to the project report file.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    module_names = {\n",
    "        1: \"Verify DICOM Files\",\n",
    "        2: \"Check Duplicate SOP Instance UIDs\",\n",
    "        3: \"Check DICOM Consistency\",\n",
    "        4: \"Verify DICOM IOD Data Consistency\",\n",
    "        5: \"Remove PHI\",\n",
    "        6: \"Generate Summary\",\n",
    "        7: \"Rename DICOM Files\",\n",
    "        8: \"Generate DME\"\n",
    "    }\n",
    "    \n",
    "    if module_number in module_names:\n",
    "        print(f\"Executing Module {module_number}: {module_names[module_number]}\")\n",
    "        if module_number == 1:\n",
    "            verify_dicom_files(directory, output_path, project_report_file)\n",
    "        elif module_number == 2:\n",
    "            check_duplicate_sop_uids(directory, output_path, project_report_file)\n",
    "        elif module_number == 3:\n",
    "            check_dicom_consistency(directory, output_path, project_report_file)\n",
    "        elif module_number == 4:\n",
    "            verify_dicom_iod_data(directory, output_path, project_report_file)\n",
    "        elif module_number == 5:\n",
    "            #PHI Removal Tool\n",
    "            aggregate_dicom_metadata(directory, output_path, project_report_file)\n",
    "        elif module_number == 6:\n",
    "            generate_summary(directory, output_path, project_report_file)\n",
    "        elif module_number == 7:\n",
    "            rename_dicom_files(directory, output_path, project_report_file)\n",
    "        elif module_number == 8:\n",
    "            generate_DME(directory, output_path, project_report_file)\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter a number between 1 and 8.\")\n",
    "\n",
    "# Function to execute all modules with option to skip\n",
    "def execute_all_modules(source_path, output_path):\n",
    "    print(\"Executing all functions in succession with option to skip each step:\")\n",
    "    for i in range(1, 9):\n",
    "        choice = input(f\"Execute module {i}? (yes/no): \")\n",
    "        if choice.lower() == 'yes':\n",
    "            execute_module(i, source_path, output_path)\n",
    "        else:\n",
    "            print(f\"Skipping module {i}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name=None, project_name=None, log_date=None, output_path=None, source_path=None):\n",
    "    \"\"\"\n",
    "    Main function to execute the DICOM processing tool.\n",
    "\n",
    "    Args:\n",
    "    - name (str): User's name.\n",
    "    - project_name (str): Name of the project.\n",
    "    - log_date (str): Date for the log file (YYYY-MM-DD).\n",
    "    - output_path (str): Output directory path.\n",
    "    - source_path (str): Path to the source DICOM files.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if not all([name, project_name, log_date, output_path, source_path]):\n",
    "        name, project_name, log_date, output_path, source_path = prompt_user()\n",
    "\n",
    "    log_file_name = f\"{project_name}_log_{log_date}.csv\"\n",
    "    log_file_path = os.path.join(output_path, log_file_name)\n",
    "\n",
    "    create_log_file(log_file_path)\n",
    "\n",
    "    project_report_name = f\"{project_name}_report_{log_date}.csv\"\n",
    "    project_report_file = os.path.join(output_path, project_report_name)\n",
    "\n",
    "    print(f\"Hello, {name}! Welcome to {project_name} DICOM Processing Tool.\")\n",
    "    print(\"Choose an option:\")\n",
    "    print(\"1. Execute individual modules\")\n",
    "    print(\"2. Execute all functions in succession with option to skip\")\n",
    "    option = int(input(\"Enter your choice (1-2): \"))\n",
    "\n",
    "    if option == 1:\n",
    "        module_number = int(input(\"Enter the module number you want to execute (1-8): \"))\n",
    "        execute_module(module_number, source_path, output_path, project_report_file)\n",
    "    elif option == 2:\n",
    "        execute_all_modules(source_path, output_path, project_report_file)\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter 1 or 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, ToS! Welcome to UCSF56 DICOM Processing Tool.\n",
      "Choose an option:\n",
      "1. Execute individual modules\n",
      "2. Execute all functions in succession with option to skip\n",
      "Executing Module 5: Remove PHI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 636it [18:58,  2.79s/it]c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (19) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (24) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (42) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "Processing folders: 642it [18:58,  1.03it/s]c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (18) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\tosullivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydicom\\valuerep.py:290: UserWarning: The value length (43) exceeds the maximum length of 16 allowed for VR SH.\n",
      "  warnings.warn(msg)\n",
      "Processing folders: 1048it [29:07,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated transposed DICOM metadata saved to: C:/Users/tosullivan/Documents/Dev/Test_output\\transposed_dicom_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "   #To run with hardcoded params\n",
    "   name = \"Name here\"\n",
    "   project_name = \"Project name here\"\n",
    "   log_date = \"YYYY-MM-DD\"\n",
    "   output_path = \"Path with forward slashes\"\n",
    "   source_path = \"Path with forward slashes\"\n",
    "   main(name, project_name, log_date, output_path, source_path)\n",
    "\n",
    "   #main()\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
